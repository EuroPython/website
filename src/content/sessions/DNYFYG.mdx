---
code: DNYFYG
delivery: in-person
duration: '30'
end: null
level: intermediate
next_talk_code: null
prev_talk_code: null
room: null
slug: fine-tuning-large-models-on-local-hardware
speakers:
- benjamin-bossan
start: null
state: confirmed
submission_type: Talk
talks_after: null
talks_in_parallel: null
title: Fine-tuning large models on local hardware
track: 'PyData: LLMs'
tweet: ''
website_url: https://ep2024.europython.eu/session/fine-tuning-large-models-on-local-hardware
---

Fine-tuning big neural nets like Large Language Models (LLMs) has traditionally been prohibitive due to high hardware requirements. However, Parameter-Efficient Fine-Tuning (PEFT) and quantization enable the training of large models on modest hardware. Thanks to the PEFT library and the Hugging Face ecosystem, these techniques are now accessible to a broad audience.

Expect to learn:

- what the challenges are of fine-tuning large models
- what solutions have been proposed and how they work
- practical examples of applying the PEFT library
