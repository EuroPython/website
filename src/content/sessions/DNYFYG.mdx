---
code: DNYFYG
delivery: in-person
duration: '30'
end: '2024-07-11T13:00:00+02:00'
level: intermediate
next_session: 9G8GWM
prev_session: RSRDBM
resources: null
room: Forum Hall
session_type: Talk
sessions_after:
- 8MGKUK
- 9G8GWM
- ALVMH3
- BUH9SD
- EN98JL
- FLJFEG
- KLXQAM
- KXF8JY
- Y3YDNQ
sessions_before:
- A3QRK3
- CMETS8
- NDUKDX
- NFCPVM
- QQMDWQ
- RSRDBM
sessions_in_parallel:
- DH3AE7
- KHTUSV
- PSGLDJ
- VFMXAD
- WKLEEW
slug: fine-tuning-large-models-on-local-hardware
speakers:
- benjamin-bossan
start: '2024-07-11T12:30:00+02:00'
title: Fine-tuning large models on local hardware
track: 'PyData: LLMs'
tweet: ''
website_url: https://ep2024.europython.eu/session/fine-tuning-large-models-on-local-hardware
---

Fine-tuning big neural nets like Large Language Models (LLMs) has traditionally been prohibitive due to high hardware requirements. However, Parameter-Efficient Fine-Tuning (PEFT) and quantization enable the training of large models on modest hardware. Thanks to the PEFT library and the Hugging Face ecosystem, these techniques are now accessible to a broad audience.

Expect to learn:

- what the challenges are of fine-tuning large models
- what solutions have been proposed and how they work
- practical examples of applying the PEFT library
