---
code: ZCXMQK
delivery: in-person
duration: '30'
end: '2023-07-20T11:00:00+02:00'
level: intermediate
next_talk: RGRQVQ
prev_talk: null
resources: null
room: South Hall 2B
slug: understanding-neural-network-architectures-with-attention-and-diffusion
speakers:
- michal-karzynski
start: '2023-07-20T10:30:00+02:00'
state: confirmed
submission_type: Talk
talks_after:
- B3MU99
- S3LKMH
- XLLSWL
- BMYYTX
- RGRQVQ
- TYTBKX
talks_before:
- ZJRTGW
talks_in_parallel:
- ZUSHKU
- SDEUCB
- U9SNQ7
- UD3GZD
- 7KAKAG
- WUPCGU
title: Understanding Neural Network Architectures with Attention and Diffusion
track: 'PyData: Deep Learning, NLP, CV (2023)'
tweet: ''
website_url: https://ep2023.europython.eu/session/understanding-neural-network-architectures-with-attention-and-diffusion
---

Neural networks have revolutionized AI, enabling machines to learn from data and make intelligent decisions. In this talk, we'll explore two popular architectures: Attention models and Diffusion models.

First up, we'll discuss Attention models and how they've contributed to the success of large language models like ChatGPT. We'll explore how the Attention mechanism helps GPT focus on specific parts of a text sequence and how this mechanism has been applied to different tasks in natural language processing.

Next, we'll dive into Diffusion models, a class of generative models that have shown remarkable performance in image synthesis. We'll explain how they work and their potential applications in the creative industry.

This is a good talk for visual learners. I prepared schematic diagrams, which present main features of the nerual network architectures. By necessity, the diagrams are oversimplified, but I believe they will allow you to gain some insight into Transformers and Latent Diffusion models.
