---
code: VBHMEL
delivery: in-person
duration: '30'
end: '2024-07-10T15:05:00+02:00'
level: intermediate
next_session: X7GUSW
prev_session: 7GRP3T
resources: null
room: Terrace 2B
session_type: Talk
sessions_after:
- NMBFTX
- SYKBQB
- TTN3RZ
- UPWH7Z
- X7GUSW
- ZSBJNR
sessions_before:
- 7GRP3T
- 7Z8LFA
- A3E3XE
- B8SZMM
- GVKEAK
- LYNADL
- M9TMMQ
- QPPRQQ
- TLHPWB
sessions_in_parallel:
- 7XMZGV
- 8NYTHE
- H8Z37Q
- KV3DHG
- LWSF9C
slug: unlocking-mixture-of-experts-from-1-know-it-all-to-group-of-jedi-masters
speakers:
- pranjal-biyani
start: '2024-07-10T14:35:00+02:00'
title: 'Unlocking Mixture of Experts : From 1 Know-it-all to group of Jedi Masters'
track: 'PyData: Deep Learning, NLP, CV'
tweet: 'At almost equal expense, who''d you hire : A group of experts OR one fairly
  knowledgeable associate ?'
website_url: https://ep2024.europython.eu/session/unlocking-mixture-of-experts-from-1-know-it-all-to-group-of-jedi-masters
---

Answer this : In critical domains like Healthcare would you prefer a Jack-of-all-trades OR one Yoda, the master?

Join me on an exhilarating journey as we delve deep into the Mixture of Experts (MoE) technique which is a practical and intuitive next-step to elevate predictive powers of generalised know-it-all models.

A powerful approach to solve a variety of ML tasks, MoE operates on the principle of Divide and Conquer with some less obvious limitations, pros, and cons. You’ll go through a captivating exploration of insights, intuitive reasoning, solid mathematical underpinnings, and a treasure trove of interesting examples!

We'll kick off by surveying the landscape, from ensemble models to stacked estimators, gradually ascending towards the pinnacle of MoE. Along the way, we'll explore challenges, alternative routes, and the crucial art of knowing when to wield the MoE magic—AND when to hold back. Brace yourselves for a business-oriented finale, where we discuss metrics around cost, latency, and throughput for MoE models. And fear not! We'll wrap up with an array of resources equipping you to dive headfirst into pre-trained MoE models, fine-tune them, or even forge your own from scratch. May the force of Experts be with you !"
