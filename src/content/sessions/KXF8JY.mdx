---
code: KXF8JY
delivery: in-person
duration: '60'
end: '2024-07-11T14:00:00+02:00'
level: intermediate
next_session: null
prev_session: null
resources:
- description: Similarity
  resource: https://pretalx.com/media/europython-2024/submissions/KXF8JY/resources/Screenshot_2024-03-06_at_9.35.37_SuuCsDd.png
room: Exhibit Hall
session_type: Poster
sessions_after:
- 8MGKUK
- 9G8GWM
- ALVMH3
- EN98JL
- FLJFEG
- Y3YDNQ
sessions_before:
- DH3AE7
- DNYFYG
- KHTUSV
- PSGLDJ
- VFMXAD
- WKLEEW
sessions_in_parallel:
- BUH9SD
- KLXQAM
slug: are-llms-smarter-in-some-languages-than-others
speakers:
- pavel-kral
start: '2024-07-11T13:00:00+02:00'
title: Are LLMs smarter in some languages than others?
track: 'PyData: LLMs'
tweet: ''
website_url: https://ep2024.europython.eu/session/are-llms-smarter-in-some-languages-than-others
youtube_url: null
---

Have you ever asked yourself if Large Language Models (LLMs) perform differently across various languages? I have.

In this poster session, I will demonstrate how tokens, embeddings, and the LLMs themselves perform when utilized in 30 different languages. I will illustrate how languages influence pricing and various model characteristics.

Spoiler:
- The Greek language is the most expensive to process by most models.
- Processing Asian languages on Gemini is cheaper.
- You can save up to 15% of tokens by removing diacritics.
