---
code: KXF8JY
delivery: in-person
duration: '60'
end: null
level: intermediate
next_talk: null
prev_talk: null
resources:
- description: Similarity
  resource: https://program.europython.eu/media/europython-2024/submissions/KXF8JY/resources/Screenshot_2024-03-06_at_9.35.37_SuuCsDd.png
room: null
slug: are-llms-smarter-in-some-languages-than-others
speakers:
- pavel-kral
start: null
state: confirmed
submission_type: Poster
talks_after: null
talks_before: null
talks_in_parallel: null
title: Are LLMs smarter in some languages than others?
track: 'PyData: LLMs'
tweet: ''
website_url: https://ep2024.europython.eu/session/are-llms-smarter-in-some-languages-than-others
---

Have you ever asked yourself if Large Language Models (LLMs) perform differently across various languages? I have.

In this poster session, I will demonstrate how tokens, embeddings, and the LLMs themselves perform when utilized in 30 different languages. I will illustrate how languages influence pricing and various model characteristics.

Spoiler:
- The Greek language is the most expensive to process by most models.
- Processing Asian languages on Gemini is cheaper.
- You can save up to 15% of tokens by removing diacritics.
