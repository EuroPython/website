---
code: FYHQUE
delivery: in-person
duration: '30'
end: '2024-07-12T16:20:00+02:00'
level: intermediate
next_session: null
prev_session: KDH3J3
resources: null
room: Terrace 2A
session_type: Talk
sessions_after:
- CWDS3S
sessions_before:
- HZSGYK
- JJAKNR
- JRGVQM
- KDH3J3
- KVNTFE
- LEMZSB
- LXYWXR
- QS8KJY
- ZXXDBV
sessions_in_parallel:
- 89RTNU
- HX9ZWH
- M9TMMQ
- N7ZC9X
- WBFDNJ
slug: taming-one-quadrillion-data-points-with-apache-iceberg-and-parquet
speakers:
- gowthami-bhogireddy
start: '2024-07-12T15:50:00+02:00'
title: Taming One Quadrillion Data Points with Apache Iceberg and Parquet
track: 'PyData: Data Engineering'
tweet: "Join @TechAtBloomberg's Gowthami Bhogireddy at #EuroPython2024 to learn how\
  \ her team unleashed the dynamic duo of #bigdata management \u2013 #ApacheIceberg\
  \ & #Parquet \u2013 to handle one quadrillion data points!"
website_url: https://ep2024.europython.eu/session/taming-one-quadrillion-data-points-with-apache-iceberg-and-parquet
---

Bloomberg is a leading provider of financial data, with financial data spanning multiple decades. Handling and organizing these huge datasets can be challenging, with typical concerns including sluggish query performance, high storage costs, and data consistency problems.

This talk will describe how Apache Iceberg and Parquet are the dynamic duo of big data management, offering ACID transactions, time travel, and columnar storage capabilities that enable lightning-fast query performance and seamless schema evolution for even our largest workloads.

The session will introduce Apache Iceberg, an open-source table format that enables incremental updates, versioning, and schema evolution. The discussion will then focus on Parquet files, which store data in a compressed and columnar format to enhance query performance and lower storage costs. Finally, the session will outline how our Enterprise Data Lake Applications engineering team has harnessed the capabilities of Apache Iceberg (especially PyIceberg) to revolutionize our data management and analytical processing workflows.

Attendees will be able to apply the best practices discussed in the talk to build better infrastructure for their growing data demands and spur innovation within their organization.
