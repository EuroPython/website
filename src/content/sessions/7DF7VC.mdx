---
code: 7DF7VC
delivery: in-person
duration: '45'
end: '2024-07-10T12:55:00+02:00'
level: advanced
next_session: M9TMMQ
prev_session: WP8MXM
resources: null
room: North Hall
session_type: Talk (long session)
sessions_after:
- 7GRP3T
- 7Z8LFA
- 8NYTHE
- A3E3XE
- B8SZMM
- LYNADL
- M9TMMQ
- QPPRQQ
- TLHPWB
sessions_before:
- K9AHYT
- LDUPVK
- LEXULB
- P93P8V
- PC3XVJ
- WP8MXM
sessions_in_parallel:
- A3EWQU
- G3PHLZ
- GVKEAK
- JFFDLS
- NKBKYC
slug: deconstructing-the-text-embedding-models
speakers:
- kacper-lukawski
start: '2024-07-10T12:10:00+02:00'
title: Deconstructing the text embedding models
track: 'PyData: Deep Learning, NLP, CV'
tweet: Tokenizers are the most underrated parts of not only LLMs but also text embedding
  models used to build semantic search apps, i.e., RAG. @LukawskiKacper will describe
  their crucial role and show how to control them!
website_url: https://ep2024.europython.eu/session/deconstructing-the-text-embedding-models
---

Selecting the optimal text embedding model is often guided by benchmarks such as the Massive Text Embedding Benchmark (MTEB). While choosing the best model from the leaderboard is a common practice, it may not always align perfectly with the unique characteristics of your specific dataset. This approach overlooks a crucial yet frequently underestimated element - the tokenizer.

We will delve deep into the tokenizer's fundamental role, shedding light on its operations and introducing straightforward techniques to assess whether a particular model is suited to your data based solely on its tokenizer. We will explore the significance of the tokenizer in the fine-tuning process of embedding models and discuss strategic approaches to optimize its effectiveness.
