---
code: 7DF7VC
delivery: in-person
duration: '45'
end: null
level: advanced
next_talk: null
prev_talk: null
resources: null
room: null
slug: deconstructing-the-text-embedding-models
speakers:
- kacper-lukawski
start: null
state: confirmed
submission_type: Talk (long session)
talks_after: null
talks_before: null
talks_in_parallel: null
title: Deconstructing the text embedding models
track: 'PyData: Deep Learning, NLP, CV'
tweet: Tokenizers are the most underrated parts of not only LLMs but also text embedding
  models used to build semantic search apps, i.e., RAG. @LukawskiKacper will describe
  their crucial role and show how to control them!
website_url: https://ep2024.europython.eu/session/deconstructing-the-text-embedding-models
---

Selecting the optimal text embedding model is often guided by benchmarks such as the Massive Text Embedding Benchmark (MTEB). While choosing the best model from the leaderboard is a common practice, it may not always align perfectly with the unique characteristics of your specific dataset. This approach overlooks a crucial yet frequently underestimated element - the tokenizer.

We will delve deep into the tokenizer's fundamental role, shedding light on its operations and introducing straightforward techniques to assess whether a particular model is suited to your data based solely on its tokenizer. We will explore the significance of the tokenizer in the fine-tuning process of embedding models and discuss strategic approaches to optimize its effectiveness.
